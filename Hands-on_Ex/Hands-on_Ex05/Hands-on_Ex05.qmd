---
title: "Hands-on Exercise 5"
author: "Eugene Toh"
execute:
  freeze: true
---

# Spatial Weights and Applications

## Importing libraries

```{r}
pacman::p_load(spdep, tmap, tidyverse)
```

## Loading of data

```{r}
hunan <- st_read(dsn = "data/geospatial", layer = "Hunan")
```

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

## Data wrangling

Now we will need to join the SFO and the CSV dataframe into one. To do that, we combine two tables into one with `left_join` (any column with the same name is overwritten) and select each row from the resulting dataframe to make a new one.

```{r}
hunan <- left_join(hunan, hunan2012)
```

## Visualisation

```{r}
#| fig-width: 12
#| fig-height: 10
basemap <- tm_shape(hunan) +
  tm_polygons() +
  tm_text("NAME_3", size=0.5)

gdppc <- qtm(hunan, "GDPPC")
tmap_arrange(basemap, gdppc, asp=1, ncol=2)
```

Remember, spatial lag is rarely used to estimate data. It is typically used to derive a clear pattern from existing data.

## Computing contiguity weight matrices

In R, the function `poly2nb()` is part of the **spdep** package, which is used for spatial data analysis. The `poly2nb()` function takes in an SFO and creates a neighbor list for a set of spatial polygons. `poly2nb()` determines which polygons are "neighbors" based on whether they touch each other (contiguity). The output is a neighbor object (class `nb`) where each polygon is assigned a list of the IDs of its neighboring polygons.

```{r}
wm_q <- poly2nb(hunan, queen=TRUE)
summary(wm_q)
```

The term **"queen"** in spatial analysis refers to a method of defining neighborhood relationships between polygons based on how they touch each other. The name comes from the movement of the queen piece in chess, which can move in any direction—horizontally, vertically, or diagonally.

**Queen contiguity** considers two polygons to be neighbors if they touch at any point, whether they share a full edge (side) or just a corner. This is a broader and more inclusive definition of neighbors compared to **rook contiguity**.

In rook contiguity, neighbour polygons must share a full edge.

```{r}
wm_r <- poly2nb(hunan, queen=FALSE)
summary(wm_r)
```

The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.

For each polygon in our polygon object, *wm_q* lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:

```{r}
wm_q[[1]]
```

We can retrive the county name of Polygon ID=1 by using the code chunk below:

```{r}
hunan$County[1]
```

To reveal the county names of the five neighboring polygons, the code chunk will be used:

```{r}
hunan$NAME_3[c(2,3,4,57,85)]
```

We can retrieve the GDPPC (GDP per capita) of these five countries by using the code chunk below.

```{r}
nb1 <- wm_q[[1]]
nb1 <- hunan$GDPPC[nb1]
nb1
```

```{r}
str(wm_q)
```

### Visualising contiguity weights

A connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs.

```{r}
longitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])
length(longitude)
longitude[1]
```

```{r}
latitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])
```

```{r}
coords <- cbind(longitude, latitude)
```

```{r}
head(coords)
```

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey", main="Queen Contiguity")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red")
plot(hunan$geometry, border="lightgrey", main="Rook Contiguity")
plot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

## Computing distance based neighbours

In this section, you will learn how to derive distance-based weight matrices by using [*dnearneigh()*](https://r-spatial.github.io/spdep/reference/dnearneigh.html) of **spdep** package. The function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in **km** will be calculated assuming the WGS84 reference ellipsoid.

### Determining the cut-off distance

```{r}
#coords <- coordinates(hunan)
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.

### Computing fixed distance weight matrix

Now, we will compute the distance weight matrix by using *dnearneigh()* as shown in the code chunk below. 0 is the lower bound and 62 is the upper bound of the distance range.

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

Next, we will use *str()* to display the content of wm_d62 weight matrix.

```{r}
str(wm_d62)
```

This returns a list of the indices of nearest neighbours for each point based on a given distance. Don't worry about the numbers surrounded by square brackets, they just tell you the indexes start from 1 onwards.

This has the problem of a centroid being less effective when the polygon is larger.

### Plotting fixed distance weight matrix

```{r}
par(mfrow=c(1,2))
plot(hunan$geometry, border="lightgrey", main="1st nearest neighbours")
plot(k1, coords, add=TRUE, col="red", length=0.08)
plot(hunan$geometry, border="lightgrey", main="Distance link")
plot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)
```

### Computing adaptive distance weight matrix

This ensures that each point has a total neighbour count of a certain number. In spatial datasets, points or regions can be distributed unevenly. For instance, urban areas may have many more observations (e.g., people, buildings, events) than rural areas (we're not just talking about centroids of polygons here). A fixed distance threshold may result in some points having too many neighbors in densely populated areas and too few neighbors in sparsely populated areas. An adaptive distance weight matrix ensures that each point has a consistent number of neighbors, irrespective of the spatial density.

```{r}
knn6 <- knn2nb(knearneigh(coords, k=6))
knn6
```

```{r}
str(knn6)
```

## Weights based on Inverse Distance Weighting (IDW)

It estimates the value at an unknown location based on the values at nearby known locations, with the assumption that points that are closer to the unknown location have more influence than points that are farther away.

```{r}
dist <- nbdists(wm_q, coords, longlat = TRUE)
ids <- lapply(dist, function(x) 1/(x))
ids
```

**`function(x) 1/(x)`**: This function takes each vector of distances (`x`) from the list `dist` and computes the **inverse** of those distances (i.e., `1 / distance`).

-   The result of applying this function is that points closer to each other will have a larger value (since the inverse of a small distance is large), and points farther away will have smaller values.

**`lapply()`**: This function applies a given function to each element of a list (`dist` in this case). It loops over each element of the list and applies the specified function.

The result (`ids`) will be a list where each element contains the inverse distances between a point and its neighbors.

Knowing the inverse distances alone isn't inherently useful unless you are applying them for a specific purpose in spatial analysis. The **inverse distance** is typically used as a weight in various spatial models or techniques, where **closer points** have **more influence** than distant ones. One drawback is that this method does not do edge correction, which means that points around the edges will have less neighbours and hence lower values.

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

To see the weight of the first polygon’s eight neighbors type:

```{r}
rswm_q$weights[10]
```

Each neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.

```         
library(spdep)

# Example: coords is your matrix of known point coordinates
# values contains the known values (e.g., temperature, pollution, etc.)
# x_0 is the location where you want to estimate the value
x_0 <- c(lon, lat)  # coordinates of the unknown point

# Calculate the distances between the unknown point and known points
distances <- spDistsN1(coords, x_0, longlat = TRUE)

# Set the power parameter for inverse distance (usually 2)
p <- 2

# Calculate inverse distances (weights)
weights <- 1 / (distances^p)

# Estimate the value at x_0 by calculating the weighted average
estimated_value <- sum(weights * values) / sum(weights)

# Print the estimated value
print(estimated_value)
```

**"W"** stands for **row-standardized weights**. This means that the weights for each row (i.e., for each observation or spatial point) are normalized so that they sum up to 1. In a row-standardized weight matrix, each element is divided by the sum of the weights for that row, ensuring that all the weights for a given point’s neighbors add up to 1.

Row-standardization is useful when you want to make the sum of weights comparable across observations. It helps in cases where some points have many neighbors and others have few, so you ensure that every point contributes equally overall.

**"B"** stands for **binary weights**. In this case, each neighbor is either given a weight of **1** (if it is a neighbor) or **0** (if it is not a neighbor). Binary weights are the simplest form of spatial weighting. Each point either has full influence on its neighbors (weight = 1), or no influence (weight = 0), without considering the distance or the number of neighbors. Binary weights are useful in cases where you are only interested in whether two points are neighbors, without differentiating between them based on distance or proximity. It is a straightforward approach for identifying and analyzing neighborhood structures. All neighbors have the same influence, regardless of how many neighbors exist or how far apart they are.

**`zero.policy = TRUE`**: This allows handling cases where some points have **no neighbors**. If `zero.policy = TRUE`, the function will handle these cases by assigning zero weights to points with no neighbors, instead of causing an error.

This alone will not give you an estimate of a value given a location. For that, you'll need spatial lagged models.

## Application of Spatial Weight Matrix

In this section, you will learn how to create four different spatial lagged variables. A spatial lagged model is where the value at a given point is influenced by a weighted average of the values of its neighbors.

### Spatial lag with row-standardised weights

Computing the average neighbor GDPPC value for each polygon.

```{r}
GDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)
GDPPC.lag
```

Each number correspond to each row in `hunan`.

We can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.

```{r}
lag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))
lag.res <- as.data.frame(lag.list)
colnames(lag.res) <- c("NAME_3", "lag GDPPC")
hunan <- left_join(hunan,lag.res)
```

The following table shows the average neighboring income values (stored in the Inc.lag object) for each county.

```{r}
head(hunan)
```

Next, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_gdppc <- qtm(hunan, "lag GDPPC")
tmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)
```

### Spatial lag as a sum of neighbouring values

```{r}
b_weights <- lapply(wm_q, function(x) 0*x + 1)
b_weights2 <- nb2listw(wm_q, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

```{r}
lag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
lag.res <- as.data.frame(lag_sum)
colnames(lag.res) <- c("NAME_3", "lag_sum GDPPC")
```

```{r}
hunan <- left_join(hunan, lag.res)
```

```{r}
gdppc <- qtm(hunan, "GDPPC")
lag_sum_gdppc <- qtm(hunan, "lag_sum GDPPC")
tmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)
```

### Spatial window average

The spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.

To add the diagonal element to the neighbour list, we just need to use *include.self()* from **spdep**.

```{r}
wm_qs <- include.self(wm_q)
wm_qs[[1]]
```

Now we obtain weights with *nb2listw():*

```{r}
wm_qs <- nb2listw(wm_qs)
wm_qs
```

Lastly, we just need to create the lag variable from our weight structure and GDPPC variable.

```{r}
lag_w_avg_gpdpc <- lag.listw(wm_qs, 
                             hunan$GDPPC)
lag_w_avg_gpdpc
```

Next, we will convert the lag variable listw object into a data.frame by using *as.data.frame()*.

```{r}
lag.list.wm_qs <- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))
lag_wm_qs.res <- as.data.frame(lag.list.wm_qs)
colnames(lag_wm_qs.res) <- c("NAME_3", "lag_window_avg GDPPC")
```

Next, the code chunk below will be used to append *lag_window_avg GDPPC* values onto *hunan* sf data.frame by using *left_join()* of **dplyr** package.

```{r}
hunan <- left_join(hunan, lag_wm_qs.res)
```

To compare the values of lag GDPPC and Spatial window average, `kable()` of Knitr package is used to prepare a table using the code chunk below.

```{r}
pacman::p_load(knitr)
```

```{r}
hunan %>%
  select("County", 
         "lag GDPPC", 
         "lag_window_avg GDPPC") %>%
  kable()
```

Lastly, *qtm()* of **tmap** package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.

```{r}
w_avg_gdppc <- qtm(hunan, "lag_window_avg GDPPC")
tmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)
```

### Spatial window sum

The spatial window sum is the counterpart of the window average, but without using row-standardized weights.

```{r}
wm_qs <- include.self(wm_q)
wm_qs
```

Next, we will assign binary weights to the neighbour structure that includes the diagonal element.

```{r}
b_weights <- lapply(wm_qs, function(x) 0*x + 1)
b_weights[1]
```

Again, we use *nb2listw()* and *glist()* to explicitly assign weight values.

```{r}
b_weights2 <- nb2listw(wm_qs, 
                       glist = b_weights, 
                       style = "B")
b_weights2
```

With our new weight structure, we can compute the lag variable with *lag.listw()*.

```{r}
w_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))
w_sum_gdppc
```

Next, we will convert the lag variable listw object into a data.frame by using *as.data.frame()*.

```{r}
w_sum_gdppc.res <- as.data.frame(w_sum_gdppc)
colnames(w_sum_gdppc.res) <- c("NAME_3", "w_sum GDPPC")
```

The second command line on the code chunk above renames the field names of *w_sum_gdppc.res* object into *NAME_3* and *w_sum GDPPC* respectively.

```{r}
hunan <- left_join(hunan, w_sum_gdppc.res)
```

```{r}
hunan %>%
  select("County", "lag_sum GDPPC", "w_sum GDPPC") %>%
  kable()
```

```{r}
w_sum_gdppc <- qtm(hunan, "w_sum GDPPC")
tmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)
```

### Which method to use?

-   Spatial Lag with Row-Standardized Weights

    -   Use this method when you want each point to have an equal influence across all of its neighbors, regardless of the number of neighbors it has.

    -   It is most appropriate when you expect the spatial relationships to be similar across your dataset (i.e., you don't want points with many neighbors to overwhelm points with fewer neighbors).

    -   Commonly used in **spatial autoregressive models** (SAR) like the **spatial lag model** (SLM).

    -   Typically used in **spatial econometrics** when you’re interested in modeling the spillover effects or dependence of a variable across space.

    -   **Example**: In a housing price study, you might use this if you want to see how the prices of neighboring houses (standardized by distance) affect the price of a given house.

-   Spatial Lag as a Sum of Neighboring Values

    -   Use this method when you want to model the **total influence** of the neighbors without diluting the effect based on the number of neighbors.

    -   It is useful when the **total volume or intensity** of the neighboring values matters, rather than their average or relative influence.

    -   Best for situations where **cumulative effects** are important, like when you're interested in understanding the total influence of surrounding areas (e.g., total population or pollution).

    -   This method might be relevant in studies where **absolute values** are important, such as environmental studies focusing on **total pollution** from neighboring regions.

    -   **Example**: In studying air pollution, you might want the **sum of pollution** from all neighboring areas rather than an average influence, since the total pollution matters.

-   Spatial Window Average

    -   Use this method when you want to calculate a **local average** around each point within a specific window size (e.g., all points within a 10 km radius).

    -   It is useful when you want to smooth the data or when you're focusing on **local averages** rather than global or cumulative effects.

    -   This is often used in **spatial smoothing** or when looking for local trends in spatial data.

    -   In epidemiology, if you’re studying the **average infection rate** within a region (based on nearby regions), you might use a spatial window average to model localized clusters of infection.

-   Spatial Window Sum

    -   Use this method when the **total quantity** or accumulation of a variable in a region is important, and you want to sum the values within a certain distance from each point.

    -   It’s useful for applications where the **cumulative total** of a variable within a region matters more than its average.

    -   This method is often used in environmental studies, resource management, or studies where **total values** in a region (like total population or total resource availability) are critical.

    -   **Example**: In disaster management, if you’re estimating the **total population** within a flood-prone zone, you’d use a spatial window sum to calculate how many people live within the danger area.

Use each method depending on whether you care about relative influence, total influence, local average, or total value.
