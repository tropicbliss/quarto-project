{
  "hash": "c3739f4f6da0d45286d41c1023631fe7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands-on Exercise 12\"\nauthor: \"Eugene Toh\"\nexecute:\n  freeze: true\n---\n\n\nPredictive modeling helps us forecast future outcomes using statistical and machine learning methods. While we're trying to predict what will happen in the future, we train these models using historical data where we already know both the outcomes and the factors that influenced them.\n\nWhen we add geography to predictive modeling (geospatial predictive modeling), we're working with a key insight: events don't happen randomly across a map. Think of it like this - businesses don't open in random locations, crimes don't occur uniformly across a city, and wildlife isn't evenly distributed through a forest. Instead, these events are influenced by various geographical factors such as:\n\n-   Physical features (like terrain and climate)\n\n-   Human infrastructure (like roads and buildings)\n\n-   Social and cultural patterns (like population density and neighborhood characteristics)\n\nBy analyzing where events happened in the past and mapping these locations against relevant geographical features, we can identify patterns and relationships that help predict where similar events are likely to occur in the future.\n\n## Installing packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, spdep, GWmodel, SpatialML, tmap, rsample, Metrics, tidyverse)\n```\n:::\n\n\n## Preparing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmdata <- read_rds(\"data/model/mdata.rds\")\n```\n:::\n\n\nThe data is divided into two parts using the `initial_split()` function from the rsample package (which is part of tidymodels):\n\n-   Training data: 65% of the original dataset, used to build and train the model\n\n-   Test data: The remaining 35%, used to evaluate how well the model performs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nresale_split <- initial_split(mdata, \n                              prop = 6.5/10,)\ntrain_data <- training(resale_split)\ntest_data <- testing(resale_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(train_data, \"data/model/train_data.rds\")\nwrite_rds(test_data, \"data/model/test_data.rds\")\n```\n:::\n\n\n## Computing correlation matrix\n\nBefore loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmdata_nogeo <- mdata %>%\n  st_drop_geometry()\ncorrplot::corrplot(cor(mdata_nogeo[, 2:17]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex12_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIf all correlation values are below 0.8, there is no sign of multicolinearity.\n\nMulticollinearity occurs when predictors are highly correlated (≥0.8), which makes the model unstable and unreliable since it can't determine which variable is truly influencing the outcome.\n\n## Retrieving the stored data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- read_rds(\"data/model/train_data.rds\")\ntest_data <- read_rds(\"data/model/test_data.rds\")\n```\n:::\n\n\n## Building a non-spatial multiple linear regression\n\nThis code below creates a linear regression model that predicts `resale_price` based on 14 different features: the apartment's physical characteristics (floor area, story height, remaining lease) and various proximity measures to amenities (like MRT stations, parks, malls, schools, etc.) using the training dataset. It is essentially trying to understand how these 14 different factors influence HDB resale prices in Singapore.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_mlr <- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nsummary(price_mlr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(price_mlr, \"data/model/price_mlr.rds\" ) \n```\n:::\n\n\n## Preparing coordinates data\n\n### Extracting coordinates data\n\nHere's how we extract the location coordinates from all our datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- st_coordinates(mdata)\ncoords_train <- st_coordinates(train_data)\ncoords_test <- st_coordinates(test_data)\n```\n:::\n\n\nLet's save our results as an RDS file for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train <- write_rds(coords_train, \"data/model/coords_train.rds\")\ncoords_test <- write_rds(coords_test, \"data/model/coords_test.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train <- read_rds(\"data/model/coords_train.rds\")\ncoords_test <- read_rds(\"data/model/coords_test.rds\")\n```\n:::\n\n\n### Dropping geometry field\n\nLet's convert our spatial data frame to a regular data frame by removing the geometry column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- train_data %>% st_drop_geometry()\n```\n:::\n\n\n## Calibrating random forest model\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using random forest function of `ranger` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nrf <- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data)\nrf\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(rf, \"data/model/rf.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf <- read_rds(\"data/model/rf.rds\")\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + storey_order + remaining_lease_mths +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_PRISCH,      data = train_data) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      10335 \nNumber of independent variables:  14 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       728602496 \nR squared (OOB):                  0.9495728 \n```\n\n\n:::\n:::\n\n\n## Calibrating geographical random forest model\n\nIn this section, you will learn how to calibrate a model to predict HDB resale price by using `grf()` of `SpatialML` package.\n\n### Calibrating using training data\n\nThis code creates a Geographically Weighted Random Forest (GW-RF) model to predict HDB `resale_price` using the same 14 features we saw earlier. `bw=55` sets the bandwidth to include the 55 nearest neighbors. `kernel=\"adaptive\"` means the model adapts its influence based on the density of data points in different locations. `coords=coords_train` specifies the geographical coordinates to account for spatial relationships\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ngwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order +\n                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +\n                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)\n```\n:::\n\n\nLet’s save the model output by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(gwRF_adaptive, \"data/model/gwRF_adaptive.rds\")\n```\n:::\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_adaptive <- read_rds(\"data/model/gwRF_adaptive.rds\")\n```\n:::\n\n\n### Predicting by using test data\n\n#### Preparing the test data\n\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data <- cbind(test_data, coords_test) %>% st_drop_geometry()\n```\n:::\n\n\n#### Predicting with test data\n\nNext, `predict.grf()` of `SpatialML` package will be used to predict the resale value by using the test data and `gwRF_adaptive` model calibrated earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_pred <- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nGRF_pred <- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n```\n:::\n\n\n#### Converting the predicting output into a data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGRF_pred <- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df <- as.data.frame(GRF_pred)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_p <- cbind(test_data, GRF_pred_df)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_p <- read_rds(\"data/model/test_data_p.rds\")\n```\n:::\n\n\n### Calculating root mean square error\n\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis. In the code chunk below, `rmse` of `Metrics` package is used to compute the RMSE.\\\nThe RMSE tells us on average how far off our predictions are from the actual prices in the same units (Singapore dollars in this case). For example, if the RMSE is 50,000, it means our model's predictions are typically off by about \\$50,000 - the lower this number, the better our model is performing at predicting HDB resale prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse(test_data_p$resale_price, test_data_p$GRF_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27302.9\n```\n\n\n:::\n:::\n\n\n### Visualising the predicted values\n\nAlternatively, scatterplot can be used to visualise the actual resale price and the predicted resale price by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex12_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nIf they are highly correlated, it means that the model fits the actual data extremely well.\n",
    "supporting": [
      "Hands-on_Ex12_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}