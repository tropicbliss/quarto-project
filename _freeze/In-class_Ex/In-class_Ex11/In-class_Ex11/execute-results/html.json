{
  "hash": "1eca84ef9cddeaf6886c98c991c58113",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class Exercise 11\"\nauthor: \"Eugene Toh\"\ndate: \"last-modified\"\nexecute:\n  freeze: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, sf, tmap, httr, performance)\n```\n:::\n\n\n`httr` allows you to crawl data from the web.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolder_path <- \"data/aspatial\"\nfile_list <- list.files(path = folder_path, pattern = \"^realis.*\\\\.csv$\", full.names = TRUE)\nrealis_data <- file_list %>% map_dfr(read_csv)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 10000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Project Name, Sale Date, Address, Type of Sale, Type of Area, Nett...\ndbl  (2): Area (SQM), Number of Units\nnum  (4): Transacted Price ($), Area (SQFT), Unit Price ($ PSF), Unit Price ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 6643 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Project Name, Sale Date, Address, Type of Sale, Type of Area, Nett...\ndbl  (1): Number of Units\nnum  (5): Transacted Price ($), Area (SQFT), Unit Price ($ PSF), Area (SQM),...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(unique(realis_data$`Property Type`))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Apartment\"             \"Condominium\"           \"Detached House\"       \n[4] \"Executive Condominium\" \"Semi-Detached House\"   \"Terrace House\"        \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale <- realis_data %>% mutate(`Sale Date` = dmy(`Sale Date`)) %>% filter(`Type of Sale` == \"Resale\" & `Property Type` == \"Condominium\")\n```\n:::\n\n\nReverse geocoding allows you to pass postal codes or addresses and it will allow you to get XY coordinates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npostcodes <- unique(condo_resale$`Postal Code`)\n```\n:::\n\n\n```r\nurl <- \"https://onemap.gov.sg/api/common/elastic/search\"\nfound <- data.frame()\nnot_found <- data.frame()\n\nfor (postcode in postcodes) {\n  query <- list('searchVal'=postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum'='1')\n  res <- GET(url, query=query)\n  if ((content(res)$found) != 0) {\n    found <- rbind(found, data.frame(content(res))[4:13])\n  } else {\n    not_found <- data.frame(postcode)\n  }\n}\n```\n\n```r\nfound <- found %>% select(c(6:8)) %>% rename(POSTAL = `results.POSTAL`, XCOORD = `results.X`, YCOORD = `results.Y`)\n```\n\n```r\ncondo_resale_geocoded <- left_join(condo_resale, found, by = c('Postal Code' = 'POSTAL'))\n```\n\n```r\ncondo_resale_sf <- st_as_sf(condo_resale_geocoded, coords = c(\"XCOORD\", \"YCOORD\"), crs = 3414)\n```\n\nIf you need to do weighted regression, you need to avoid overlapping points, since there are places with the same postal code.\n\n```r\noverlapping_points <- condo_resale_sf %>% mutate(overlap = lengths(st_equals(., .)) > 1)\n```\n\nIf there is overlapping, you should do spatial jittering by shifting each coordinate point randomly by 2 metres. Do not use too low values to avoid rounding.\n\n```r\ncondo_resale_sf <- condo_resale_sf %>% st_jitter(amount = 2)\n```\n\nIn take-home exercise 2:\n\nIf you take the islands into account, and a province contains islands, the centroids might drift into the sea for example. Hence, you might want to convert each row into polygons from multi-polygons. You might then result in multiple rows that refer to the same province, so you can keep the polygon that is the largest for each province.\n\n``` r\nsf_polygon <- prov_sf %>%\n  st_cast(\"POLYGON\") %>%\n  mutate(area = st_area(.))\n```\n\n``` r\nprov_cleaned <- sf_polygon %>%\n  group_by(ADM1_EN) %>%\n  filter(area == max(area)) %>%\n  ungroup() %>%\n  select(-area) %>%\n  select(ADM1_EN)\n```\n\nThis method would not remove Phuket, but other islands would be removed.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}